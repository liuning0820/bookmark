{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About DevOps https://www.dynatrace.com/ Mooc fosdem-free-event-for-dev","title":"About"},{"location":"#about","text":"","title":"About"},{"location":"#devops","text":"https://www.dynatrace.com/","title":"DevOps"},{"location":"#mooc","text":"fosdem-free-event-for-dev","title":"Mooc"},{"location":"bookmark/","text":"All-in-One Bookmarks Maintains all frequently used bookmarks in code and host on github page, set as home page on every browsers on every devices for personal use. it is cross browsers and keep in sync with devices. Bookmark Sync Solution Xmarks Bookmarks sync between browsers ChromeExtension \"Bookmarks Anywhere\" and access using html. An html format bookmark file be set as home page across devices and sync between browsers. When saved in OneDrive or GitHub, can sync the bookmarks across devices and network. \u8de8\u8bbe\u5907\u8de8\u6d4f\u89c8\u5668+ \u4e66\u7b7e\u5373\u4ee3\u7801 \u7528HTML\u4ee3\u7801\u7684\u5f62\u5f0f\u7ef4\u62a4\u7ba1\u7406\u5e38\u7528\u7f51\u5740\uff0c\u7528Git\u8ddf\u8e2a\uff0c\u5e76\u6258\u7ba1\u5728GitHub Page\u4f5c\u4e3a\u5355\u9875\u9762\u7f51\u9875\u3002 \u5728\u6211\u7684iPhone, iPad, Windows PC, Ubuntu \u673a\u5668\u4e0a\u9762\u5404\u79cd\u6d4f\u89c8\u5668\u8bbe\u7f6e\u8fd9\u4e2a\u9875\u9762\u4e3a\u4e3b\u9875\u3002 \u8fd9\u6837\u53ef\u4ee5\u968f\u65f6\u968f\u5730\u8bbf\u95ee\u5230\u6211\u7684\u5e38\u7528\u7f51\u5740\uff0c\u5e76\u4fdd\u6301\u540c\u6b65\u3002 \u5982\u679c\u9047\u5230\u65b0\u7684\u6709\u8da3\u7684\u7f51\u5740\uff0c\u53ef\u4ee5\u968f\u65f6\u4ee5\u4ee3\u7801\u7684\u5f62\u5f0f\u63d0\u4ea4\u5230GitHub\u4e0a\uff0c\u66f4\u65b0\u4f1a\u5728\u4efb\u4f55\u5730\u65b9\u4ea7\u751f\u4f5c\u7528\u3002","title":"All-in-One Bookmarks"},{"location":"bookmark/#all-in-one-bookmarks","text":"Maintains all frequently used bookmarks in code and host on github page, set as home page on every browsers on every devices for personal use. it is cross browsers and keep in sync with devices.","title":"All-in-One Bookmarks"},{"location":"bookmark/#bookmark-sync-solution","text":"Xmarks Bookmarks sync between browsers ChromeExtension \"Bookmarks Anywhere\" and access using html. An html format bookmark file be set as home page across devices and sync between browsers. When saved in OneDrive or GitHub, can sync the bookmarks across devices and network.","title":"Bookmark Sync Solution"},{"location":"bookmark/#_1","text":"\u7528HTML\u4ee3\u7801\u7684\u5f62\u5f0f\u7ef4\u62a4\u7ba1\u7406\u5e38\u7528\u7f51\u5740\uff0c\u7528Git\u8ddf\u8e2a\uff0c\u5e76\u6258\u7ba1\u5728GitHub Page\u4f5c\u4e3a\u5355\u9875\u9762\u7f51\u9875\u3002 \u5728\u6211\u7684iPhone, iPad, Windows PC, Ubuntu \u673a\u5668\u4e0a\u9762\u5404\u79cd\u6d4f\u89c8\u5668\u8bbe\u7f6e\u8fd9\u4e2a\u9875\u9762\u4e3a\u4e3b\u9875\u3002 \u8fd9\u6837\u53ef\u4ee5\u968f\u65f6\u968f\u5730\u8bbf\u95ee\u5230\u6211\u7684\u5e38\u7528\u7f51\u5740\uff0c\u5e76\u4fdd\u6301\u540c\u6b65\u3002 \u5982\u679c\u9047\u5230\u65b0\u7684\u6709\u8da3\u7684\u7f51\u5740\uff0c\u53ef\u4ee5\u968f\u65f6\u4ee5\u4ee3\u7801\u7684\u5f62\u5f0f\u63d0\u4ea4\u5230GitHub\u4e0a\uff0c\u66f4\u65b0\u4f1a\u5728\u4efb\u4f55\u5730\u65b9\u4ea7\u751f\u4f5c\u7528\u3002","title":"\u8de8\u8bbe\u5907\u8de8\u6d4f\u89c8\u5668+ \u4e66\u7b7e\u5373\u4ee3\u7801"},{"location":"devops/sre-outage-2023/","text":"Outage DNS Outage on 2023-01-25 Root Cause of the Outage Resolution Postmortem DNS Outage on 2023-01-25 On Wednesday, 2023-01-25 at 09:15 UTC, we deployed changes to the production infrastructure for crates.io. During the deployment, the DNS record for static.crates.io failed to resolve for an estimated time of 10-15 minutes. Users experienced build failures during this time, because crates could not be downloaded. Around 9:30 UTC, the DNS record started to get propagated again and by 9:40 UTC traffic had returned to normal levels. Root Cause of the Outage The Rust infrastructure is managed with Terraform, a tool to configure and provision infrastructure-as-code. The Infrastructure team recently made changes to this configuration to separate the staging and production environments for crates.io so that both can be deployed independently of each other. This feature was used to develop and test the infrastructure for a second Content Delivery Network (CDN) for static.crates.io in the staging environment. When the configuration was ready, we scheduled and announced the rollout for January 25th. The deployment to production contained two changes that were developed, deployed, and tested individually on staging: a new TLS certificate for the current Content Delivery Network and updated DNS records. When we deployed this configuration to production, Terraform first removed the current certificate and DNS records. It then started to issue a new certificate, which took around 10 minutes. During this time, there was no DNS record for static.crates.io and users experienced build failures. After the new certificate was provisioned, Terraform recreated the DNS records. Resolution The outage resolved itself after Terraform finished the deployment and created a new DNS record for static.crates.io. For some users, the outage lasted a few minutes longer due to caches in their DNS server. Postmortem The outage could have been avoided by deploying the changes to the TLS certificate and DNS records individually. We have identified two reasons why this did not happen as well as lessons that we are taking from this. This was one of the first times that we used the new tooling around environments to deploy changes to production. One of its features is that the production environment is locked to a specific Git commit. When deploying in the past, we set this to the latest commit on master. This was done here as well, with the consequence that the deployment applied multiple changes simultaneously. Another way to look at this is that production and staging diverged too much over time, because we did not deploy the changes when we merged them into the main branch. If we had deployed the changes when they were merged into the main branch, we would have isolated the DNS change. But given the importance of crates.io to the Rust ecosystem, we were hesitant to deploy multiple times without announcing the changes to the community first. The lessons that we are taking away from this incident are as follows: We need to document the process of deploying changes to production, in particular how to pick the Git commit and how to review the changeset. Defining a process will enable us to iterate and improve it over time, and avoid the same issue in the future. Changes that have been developed and tested in isolation on staging should be deployed individually and in sequence to production. We need to add this to the documentation. When we merge changes into the main branch, we need to ensure that they get deployed to production as well. This avoids a drift between the configuration in Git and what is deployed. https://blog.rust-lang.org/inside-rust/2023/02/08/dns-outage-portmortem.html","title":"Outage"},{"location":"devops/sre-outage-2023/#outage","text":"DNS Outage on 2023-01-25 Root Cause of the Outage Resolution Postmortem","title":"Outage"},{"location":"devops/sre-outage-2023/#dns-outage-on-2023-01-25","text":"On Wednesday, 2023-01-25 at 09:15 UTC, we deployed changes to the production infrastructure for crates.io. During the deployment, the DNS record for static.crates.io failed to resolve for an estimated time of 10-15 minutes. Users experienced build failures during this time, because crates could not be downloaded. Around 9:30 UTC, the DNS record started to get propagated again and by 9:40 UTC traffic had returned to normal levels.","title":"DNS Outage on 2023-01-25"},{"location":"devops/sre-outage-2023/#root-cause-of-the-outage","text":"The Rust infrastructure is managed with Terraform, a tool to configure and provision infrastructure-as-code. The Infrastructure team recently made changes to this configuration to separate the staging and production environments for crates.io so that both can be deployed independently of each other. This feature was used to develop and test the infrastructure for a second Content Delivery Network (CDN) for static.crates.io in the staging environment. When the configuration was ready, we scheduled and announced the rollout for January 25th. The deployment to production contained two changes that were developed, deployed, and tested individually on staging: a new TLS certificate for the current Content Delivery Network and updated DNS records. When we deployed this configuration to production, Terraform first removed the current certificate and DNS records. It then started to issue a new certificate, which took around 10 minutes. During this time, there was no DNS record for static.crates.io and users experienced build failures. After the new certificate was provisioned, Terraform recreated the DNS records.","title":"Root Cause of the Outage"},{"location":"devops/sre-outage-2023/#resolution","text":"The outage resolved itself after Terraform finished the deployment and created a new DNS record for static.crates.io. For some users, the outage lasted a few minutes longer due to caches in their DNS server.","title":"Resolution"},{"location":"devops/sre-outage-2023/#postmortem","text":"The outage could have been avoided by deploying the changes to the TLS certificate and DNS records individually. We have identified two reasons why this did not happen as well as lessons that we are taking from this. This was one of the first times that we used the new tooling around environments to deploy changes to production. One of its features is that the production environment is locked to a specific Git commit. When deploying in the past, we set this to the latest commit on master. This was done here as well, with the consequence that the deployment applied multiple changes simultaneously. Another way to look at this is that production and staging diverged too much over time, because we did not deploy the changes when we merged them into the main branch. If we had deployed the changes when they were merged into the main branch, we would have isolated the DNS change. But given the importance of crates.io to the Rust ecosystem, we were hesitant to deploy multiple times without announcing the changes to the community first. The lessons that we are taking away from this incident are as follows: We need to document the process of deploying changes to production, in particular how to pick the Git commit and how to review the changeset. Defining a process will enable us to iterate and improve it over time, and avoid the same issue in the future. Changes that have been developed and tested in isolation on staging should be deployed individually and in sequence to production. We need to add this to the documentation. When we merge changes into the main branch, we need to ensure that they get deployed to production as well. This avoids a drift between the configuration in Git and what is deployed. https://blog.rust-lang.org/inside-rust/2023/02/08/dns-outage-portmortem.html","title":"Postmortem"},{"location":"devops/sre/","text":"SRE Road What's SRE SRE and DevOps Shared practices SRE Key principle SRE's Mission SLA \\& SLO \\& SLI The Four Golden Signals SRE Budgets SRE Role Expectation Observability Metrics Logs Trace What should be monitored Alert \\& Incident Response SREcon22 SREcon22 APAC - Move Fast and Learn Things: Principles of Cognition, Teaming, and Coordination Resource What's SRE Site Reliability Engineering is an engineering discipline devoted to helping an organization achieve the appropriate level of reliability in their systems, services, and products. SRE and DevOps Shared practices Automation Monitoring, Observability, Measure SRE Key principle Build feedback loop that help continuously get better. health check indicators (SLI, SLO, error budget) Blameless postmortem Eliminate toil when it is appropriate SRE's Mission Reliability - Meet the availability targets the users need Velocity - Maximize the long-term feature velocity. Maintainability - \u6613\u4e8e\u7ef4\u62a4 Efficiency - \u9ad8\u6548\u5229\u7528\u4eba\u529b\u548c\u8d44\u6e90 SLA & SLO & SLI SLA - a promise, economic incentive, or else, we get paged SLO - a goal. SLI- Service Level Indicator reference: https://devops.com/sres-stop-asking-your-product-managers-for-slos/ The Four Golden Signals Focus on metrics that tell you insights into the \u201cThe Four Golden Signals\u201d of monitoring: Latency : The time it takes to service a request. Traffic : How much demand is being placed on your system. Errors : The rate of failures in your system. Saturation : How \u201cfull\u201d is your system \u2014 and critically, how much capacity remains. SRE Budgets Performance budgets Error Budget A service that\u2019s 99.99% available is 0.01% unavailable. That permitted 0.01% unavailability is the service\u2019s error budget. Training Budget Toil budget SRE Role Expectation Set up the platform level monitoring to keep critical applications up and running despite bandwidth outage, and configuration problems. Engage in service capacity planning, software performance analysis and system tunning Collaborate with Infra team to lead platform level operation optimization \uff08AWS\uff0cAkamai\uff0cF5\uff0cDockers\uff0cDB\uff09 Conduct on call duties for critical platform issues Observability Metrics time series databases like Prometheus, InfluxDb,Graphite. Logs Loki, Splunk Trace Grafana Tempo with open trace protocal OpenTelemetry Zipkin \u2014 \u5fae\u670d\u52a1\u94fe\u8def\u8ddf\u8e2a https://zipkin.io/pages/quickstart.html Jaeger OpenTelemetry https://opentelemetry.io/ What should be monitored Service downtime (health check) Slow response time Errors and crashes events/logs Resource usage. Don't waste money on unnecessary resources ( cloud resource watch) https://hackernoon.com/node-js-monitoring-done-right-70418ecbbff9 Alert & Incident Response https://engineering.razorpay.com/what-goes-behind-managing-production-alerts-204f186ce865 SREcon22 SREcon22 APAC - Move Fast and Learn Things: Principles of Cognition, Teaming, and Coordination Cognitive Work Resource https://github.com/linkedin/school-of-sre Site Reliability Engineering: How Google Runs Production Systems (known as \"The SRE Book\") The Site Reliability Workbook: Practical Ways to Implement SRE (known as \"The SRE Workbook\") Seeking SRE: Conversations About Running Production Systems at Scale SREcon","title":"SRE Road"},{"location":"devops/sre/#sre-road","text":"What's SRE SRE and DevOps Shared practices SRE Key principle SRE's Mission SLA \\& SLO \\& SLI The Four Golden Signals SRE Budgets SRE Role Expectation Observability Metrics Logs Trace What should be monitored Alert \\& Incident Response SREcon22 SREcon22 APAC - Move Fast and Learn Things: Principles of Cognition, Teaming, and Coordination Resource","title":"SRE Road"},{"location":"devops/sre/#whats-sre","text":"Site Reliability Engineering is an engineering discipline devoted to helping an organization achieve the appropriate level of reliability in their systems, services, and products.","title":"What's SRE"},{"location":"devops/sre/#sre-and-devops-shared-practices","text":"Automation Monitoring, Observability, Measure","title":"SRE and DevOps Shared practices"},{"location":"devops/sre/#sre-key-principle","text":"Build feedback loop that help continuously get better. health check indicators (SLI, SLO, error budget) Blameless postmortem Eliminate toil when it is appropriate","title":"SRE Key principle"},{"location":"devops/sre/#sres-mission","text":"Reliability - Meet the availability targets the users need Velocity - Maximize the long-term feature velocity. Maintainability - \u6613\u4e8e\u7ef4\u62a4 Efficiency - \u9ad8\u6548\u5229\u7528\u4eba\u529b\u548c\u8d44\u6e90","title":"SRE's Mission"},{"location":"devops/sre/#sla-slo-sli","text":"SLA - a promise, economic incentive, or else, we get paged SLO - a goal. SLI- Service Level Indicator reference: https://devops.com/sres-stop-asking-your-product-managers-for-slos/","title":"SLA &amp; SLO &amp; SLI"},{"location":"devops/sre/#the-four-golden-signals","text":"Focus on metrics that tell you insights into the \u201cThe Four Golden Signals\u201d of monitoring: Latency : The time it takes to service a request. Traffic : How much demand is being placed on your system. Errors : The rate of failures in your system. Saturation : How \u201cfull\u201d is your system \u2014 and critically, how much capacity remains.","title":"The Four Golden Signals"},{"location":"devops/sre/#sre-budgets","text":"Performance budgets Error Budget A service that\u2019s 99.99% available is 0.01% unavailable. That permitted 0.01% unavailability is the service\u2019s error budget. Training Budget Toil budget","title":"SRE Budgets"},{"location":"devops/sre/#sre-role-expectation","text":"Set up the platform level monitoring to keep critical applications up and running despite bandwidth outage, and configuration problems. Engage in service capacity planning, software performance analysis and system tunning Collaborate with Infra team to lead platform level operation optimization \uff08AWS\uff0cAkamai\uff0cF5\uff0cDockers\uff0cDB\uff09 Conduct on call duties for critical platform issues","title":"SRE Role Expectation"},{"location":"devops/sre/#observability","text":"","title":"Observability"},{"location":"devops/sre/#metrics","text":"time series databases like Prometheus, InfluxDb,Graphite.","title":"Metrics"},{"location":"devops/sre/#logs","text":"Loki, Splunk","title":"Logs"},{"location":"devops/sre/#trace","text":"Grafana Tempo with open trace protocal OpenTelemetry Zipkin \u2014 \u5fae\u670d\u52a1\u94fe\u8def\u8ddf\u8e2a https://zipkin.io/pages/quickstart.html Jaeger OpenTelemetry https://opentelemetry.io/","title":"Trace"},{"location":"devops/sre/#what-should-be-monitored","text":"Service downtime (health check) Slow response time Errors and crashes events/logs Resource usage. Don't waste money on unnecessary resources ( cloud resource watch) https://hackernoon.com/node-js-monitoring-done-right-70418ecbbff9","title":"What should be monitored"},{"location":"devops/sre/#alert-incident-response","text":"https://engineering.razorpay.com/what-goes-behind-managing-production-alerts-204f186ce865","title":"Alert &amp; Incident Response"},{"location":"devops/sre/#srecon22","text":"","title":"SREcon22"},{"location":"devops/sre/#srecon22-apac-move-fast-and-learn-things-principles-of-cognition-teaming-and-coordination","text":"Cognitive Work","title":"SREcon22 APAC - Move Fast and Learn Things: Principles of Cognition, Teaming, and Coordination"},{"location":"devops/sre/#resource","text":"https://github.com/linkedin/school-of-sre Site Reliability Engineering: How Google Runs Production Systems (known as \"The SRE Book\") The Site Reliability Workbook: Practical Ways to Implement SRE (known as \"The SRE Workbook\") Seeking SRE: Conversations About Running Production Systems at Scale SREcon","title":"Resource"}]}